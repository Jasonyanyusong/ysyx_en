[#](#The-Opening-Chapter) The Opening Chapter
=====================

The Pioneer wanted to create a world of computers and gave it the mission of executing programs. Let's help him and experience the joy of creation.

As you know from your programming classes, a program is made up of code and data. For example, a program to find `1+2+... +100', you can write a program to do this without much effort. It's easy to understand that the data is what the program is dealing with, and the code describes what the program wants to do with the data. Not to mention the complex games, what would the simplest computer look like in order to execute even the simplest program?

### [#](#The-simplest-computer) The simplest computer.

In order to execute a program, the first problem to be solved is where to put it. Obviously, we don't want to create a computer that can only execute small programs. Therefore, we need a large enough component to hold a wide variety of programs, and that component is memory. So, the pioneers created memory, and put programs in memory, waiting for the CPU to execute them.

Wait, who's the CPU? You've probably heard of it before, but let's reintroduce it now. The CPU is the greatest creation of the pioneers, and its Chinese name "中央处理器" (Centrally Processing Unit) shows that it has been bestowed with the highest honor: the CPU is the core unit of circuitry responsible for processing the data, i.e., the execution of the program is all dependent on it. But a computer with only memory cannot perform calculations. Naturally, the CPU had to take on the task of computing, and the pioneers created operators for the CPU so that data could be processed in a variety of ways. If operators are too complicated, consider an adder.

Pioneer found that sometimes a program needs to process the same data in a continuous manner. For example, to calculate `1+2+... +100`, you have to add the number and `sum`, and it would be inconvenient to write it back to memory every time you finish adding, and then read it out of memory to add again. At the same time there is no free lunch, the large capacity of the memory is also necessary to pay the corresponding price, that is, slow, which is the pioneer can not violate the law of material properties. So the pioneers created registers for the CPU, allowing the CPU to temporarily store the data being processed in them.

Registers are fast, but small, and complement the properties of memory, so there may be a new story to be told between them, but for now, let's just go with the flow.

#### Can a computer have no registers? (Suggested thinking for the second round)

Can a computer work without registers? And if so, how does this affect the programming model provided by the hardware?

Even if you've been thinking about this for two weeks, it's possible that this is the first time you've heard of the concept of a "programming model". But if you've read the ISA Handbook carefully in round 1, you'll remember that there is such a concept. So, if you want to know what a programming model is, RTFM it.

In order to make the mighty CPU a loyal servant, the pioneers also designed "instructions", which instructed the CPU what to do with the data. In this way, we can control the CPU through instructions and make it do what we want it to do.

With instructions, Pioneer came up with an epoch-making idea: can we let the program automatically control the execution of the computer? In order to realize this idea, the pioneers and the CPU made a simple agreement: after executing an instruction, it would continue to execute the next instruction. But how does the CPU know which instruction has been executed? For this reason, the pioneers created a special counter for the CPU, called "Program Counter" (PC). In x86, it has a special name, called `EIP` (Extended Instruction Pointer).

From now on, the computer only has to do one thing: 

    while (1) {
      retrieve the instruction from the memory location indicated by the PC.
      Execute the instruction.
      update the PC.
    }
    

Thus, we have a simple enough computer. All we have to do is to place a sequence of instructions in memory, and let the PC point to the first instruction, and the computer will automatically execute this sequence of instructions, without ever stopping.

For example, the following sequence of instructions calculates `1+2+... +100`, where `r1` and `r2` are two registers, and there is an implicit program counter `PC`, whose initial value is `0`. To help you understand, we have translated the semantics of the instructions into C code on the right, where each line of C code is preceded by a statement tag

    // PC: instruction    | // label: statement
    0: mov  r1, 0         |  pc0: r1 = 0;
    1: mov  r2, 0         |  pc1: r2 = 0;
    2: addi r2, r2, 1     |  pc2: r2 = r2 + 1;
    3: add  r1, r1, r2    |  pc3: r1 = r1 + r2;
    4: blt  r2, 100, 2    |  pc4: if (r2 < 100) goto pc2;   // branch if less than
    5: jmp 5              |  pc5: goto pc5;
    

The computer executes the above sequence of instructions, and the last instruction at `PC=5` is in a dead loop, at which point the calculation is finished, and the result of `1+2+...` The result of `1+2+... +100` is stored in register `r1`.

#### Trying to understand how computers calculate

Before seeing the above example, you might have thought that instructions were a mysterious and difficult concept to understand. But when you look at the corresponding C code, you realize that instructions do something so simple! It's also a bit silly, as you can write a for loop that looks more advanced than this C code.

But put yourself in the shoes of a computer, and you'll see how it's possible for a computer to calculate `1+2+... + 100'. This understanding will give you an initial idea of how a program works on a computer.

This fully automated process is wonderful! In fact, Turing, the pioneer, had already formulated [a similar core idea](https://en.wikipedia.org/wiki/Universal_Turing_machine) in 1936, making him the "father of the computer". The core idea, which has been passed down to this day, is the "stored program". In honor of Turing, we call the simplest computer above the "Turing Machine" (TRM). Perhaps you have already heard of the concept of "Turing Machine" as a model of computation, but here we will only emphasize the conditions that need to be fulfilled by the simplest real computer: 

* structurally, the TRM has memory, a PC, registers, and adders
* the way it works, the TRM repeats the following process over and over again: it takes an instruction from the memory location indicated by the PC, executes the instruction, and then updates the PC.

Huh? Memory, counters, registers, adders, aren't these the same parts you learned about in digital circuits class? You may find this hard to believe, but the computer you're looking at, the computer that does everything, is made of digital circuits! But the programs we wrote in programming class were in C code. if a computer is really a giant digital circuit that only understands zeros and ones, how can this cold circuit understand the C code that is the fruit of human ingenuity? The pioneer said that in the early years of computers, there was no C language, and everyone wrote machine instructions that were obscure and difficult for humans to understand, and that was the earliest way he had ever seen to program a computer. Later on, people invented high-level languages and compilers, which can take the high-level language code we write and process it in various ways, and finally generate functionally equivalent instructions that the CPU can understand. When the CPU executes these instructions, it is executing the code we wrote. Today's computers are still essentially "stored programs", a naturally obtuse way of working, and it's only through the efforts of countless computer scientists that we are able to use computers today with ease.

#### A computer is a state machine.

Since a computer is an assembly of logic circuits, we can divide the computer into two parts, one consisting of all the sequential logic components (memories, counters, registers), and the other consisting of the remaining combinational logic components (e.g., adders, etc.). In this way, we can understand the process of a computer from the perspective of the state machine model: at each clock cycle, the computer calculates and transfers to a new state for the next clock cycle based on the current state of the timing logic components, and the action of the combinational logic components.

What's the point of this view of the computer? Well, it doesn't seem to do much except make you realize that computer hardware isn't so mysterious. After all, ICS classes don't require you to implement computer hardware in a hardware description language, you just have to believe that it can be done.

But for programs, this perspective can be more useful than you might think.

### [#](#重新认识程序-程序是个状态机) 重新认识程序: 程序是个状态机

如果把计算机看成一个状态机, 那么运行在计算机上面的程序又是什么呢?

我们知道程序是由指令构成的, 那么我们先看看一条指令在状态机的模型里面是什么. 不难理解, 计算机正是通过执行指令的方式来改变自身状态的, 比如执行一条加法指令, 就可以把两个寄存器的值相加, 然后把结果更新到第三个寄存器中; 如果执行一条跳转指令, 就会直接修改PC的值, 使得计算机从新PC的位置开始执行新的指令. 所以在状态机模型里面, 指令可以看成是计算机进行一次状态转移的输入激励.

ICS课本的1.1.3小节中介绍了一个很简单的计算机. 这个计算机有4个8位的寄存器, 一个4位PC, 以及一段16字节的内存(也就是存储器), 那么这个计算机可以表示比特总数为`B = 4*8 + 4 + 16*8 = 164`, 因此这个计算机总共可以有`N = 2^B = 2^164`种不同的状态. 假设这个在这个计算机中, 所有指令的行为都是确定的, 那么给定`N`个状态中的任意一个, 其转移之后的新状态也是唯一确定的. 一般来说`N`非常大, 下图展示了`N=50`时某计算机的状态转移图.

![state-machine](/docs/assets/state-machine.9a26446a.png)

现在我们就可以通过状态机的视角来解释"程序在计算机上运行"的本质了: 给定一个程序, 把它放到计算机的内存中, 就相当于在状态数量为`N`的状态转移图中指定了一个初始状态, 程序运行的过程就是从这个初始状态开始, 每执行完一条指令, 就会进行一次确定的状态转移. 也就是说, 程序也可以看成一个状态机! 这个状态机是上文提到的大状态机(状态数量为`N`)的子集.

例如, 假设某程序在上图所示的计算机中运行, 其初始状态为左上角的8号状态, 那么这个程序对应的状态机为

    8->1->32->31->32->31->...
    

这个程序可能是:

    // PC: instruction    | // label: statement
    0: addi r1, r2, 2     |  pc0: r1 = r2 + 2;
    1: subi r2, r1, 1     |  pc1: r2 = r1 - 1;
    2: nop                |  pc2: ;  // no operation
    3: jmp 2              |  pc3: goto pc2;
    

#### 从状态机视角理解程序运行

以上一小节中`1+2+...+100`的指令序列为例, 尝试画出这个程序的状态机.

这个程序比较简单, 需要更新的状态只包括`PC`和`r1`, `r2`这两个寄存器, 因此我们用一个三元组`(PC, r1, r2)`就可以表示程序的所有状态, 而无需画出内存的具体状态. 初始状态是`(0, x, x)`, 此处的`x`表示未初始化. 程序`PC=0`处的指令是`mov r1, 0`, 执行完之后`PC`会指向下一条指令, 因此下一个状态是`(1, 0, x)`. 如此类推, 我们可以画出执行前3条指令的状态转移过程:

    (0, x, x) -> (1, 0, x) -> (2, 0, 0) -> (3, 0, 1)
    

请你尝试继续画出这个状态机, 其中程序中的循环只需要画出前两次循环和最后两次循环即可.

通过上面必做题的例子, 你应该更进一步体会到"程序是如何在计算机上运行"了. 我们其实可以从两个互补的视角来看待同一个程序:

*   一个是以代码(或指令序列)为表现形式的静态视角, 大家经常说的"写程序"/"看代码", 其实说的都是这个静态视角. 这个视角的一个好处是描述精简, 分支, 循环和函数调用的组合使得我们可以通过少量代码实现出很复杂的功能. 但这也可能会使得我们对程序行为的理解造成困难.
*   另一个是以状态机的状态转移为运行效果的动态视角, 它直接刻画了"程序在计算机上运行"的本质. 但这一视角的状态数量非常巨大, 程序代码中的所有循环和函数调用都以指令的粒度被完全展开, 使得我们难以掌握程序的整体语义. 但对于程序的局部行为, 尤其是从静态视角来看难以理解的行为, 状态机视角可以让我们清楚地了解相应的细节.

#### 程序的状态机视角有什么好处?

有一些程序看上去很简单, 但行为却不那么直观, 比如递归. 要很好地理解递归程序在计算机上如何运行, 从状态机视角来看程序行为才是最有效的做法, 因为这一视角可以帮助你理清每一条指令究竟如何修改计算机的状态, 从而实现宏观上的递归语义. ICS理论课的第三章会专门分析其中的细节, 我们在这里就不展开讨论递归的具体行为了.

#### "程序在计算机上运行"的微观视角: 程序是个状态机

"程序是个状态机"这一视角对ICS和PA来说都是非常重要的, 因为"理解程序如何在计算机上运行"就是ICS和PA的根本目标. 至于这个问题的宏观视角, 我们将会在PA的中期来介绍.

这一小节的文字对你来说应该不难理解, 但如果你将来没有养成从状态机视角理解程序行为的意识, 你可能会感到PA非常困难, 因为在PA中你需要不断地和代码打交道. 如果你不能从微观视角理解某些关键代码的行为, 你也无法从宏观视角完全弄清楚程序究竟是如何运行的.

[在开始愉快的PA之旅之前](/docs/ics-pa/1.1.html) [RTFSC](/docs/ics-pa/1.3.html)
